{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "92bfc3548baa6bf47056a68e0b67ae206554386d09af1748009e58de9bd0f0fe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances, euclidean_distances\n",
    "\n",
    "from gensim.models import KeyedVectors, word2vec\n",
    "\n",
    "from dataset import BibleDataset, custom_collate_fn\n",
    "from model import AspectAutoencoder\n",
    "from loss import TripletMarginCosineLoss, OrthogonalityLoss\n",
    "from train_wv import get_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "home_path = os.path.expanduser('~')\n",
    "if not os.path.exists(os.path.join(home_path, 'nltk_data/corpora/stopwords')):\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BibleDataset(\"./t_kjv.csv\", word_to_index=True)\n",
    "dataloader = DataLoader(dataset, batch_size=50, shuffle=True, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "verses = [verse_tuple[0] for verse_tuple in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_kjv_sents = []\n",
    "for verse in verses:\n",
    "    verse_list = [dataset.id2word[verse_idx] for verse_idx in verse]\n",
    "    bible_kjv_sents.append(verse_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "load word bible embeddings from w2v/bible_word2vec_org \n"
     ]
    }
   ],
   "source": [
    "bible_wv = get_wv(bible_kjv_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "bible_wv['jesus'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/12530 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "624e2e45ec4a41ad83582064fc03d2e3"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "\"\"\" Generate Word Vectors for a Given Dataset \"\"\"\n",
    "\n",
    "embed_size = bible_wv.vector_size\n",
    "vocab_size = len(dataset.word2id.keys())\n",
    "wv = KeyedVectors(vector_size=embed_size)\n",
    "\n",
    "embeddings = torch.empty(vocab_size, embed_size)\n",
    "\n",
    "for word, id in tqdm(dataset.word2id.items()):\n",
    "    embeddings[id] = torch.from_numpy(bible_wv[word]).to(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial aspects via K means:\nAspect 01:  ['overlaid', 'spoons', 'hooks', 'dishes', 'knops', 'pins', 'hanging', 'skins', 'crisping', 'chapiters']\nAspect 02:  ['shuham', 'kibzaim', 'dimnah', 'mahali', 'shuhamites', 'masrekah', 'camon', 'jezerites', 'nahalal', 'archevites']\nAspect 03:  ['maaseiah', 'aziel', 'asaiah', 'uzzi', 'jashobeam', 'jehonathan', 'azaziah', 'shemiramoth', 'shelemiah', 'shimri']\nAspect 04:  ['jews', 'astrologers', 'paul', 'antioch', 'rehoboam', 'pharisees', 'junia', 'daniel', 'andronicus', 'governor']\nAspect 05:  ['zebina', 'nahath', 'zithri', 'muppim', 'abihud', 'addar', 'rosh', 'jashubites', 'allon', 'sharar']\nAspect 06:  ['year', 'seventh', 'month', 'eighth', 'thirtieth', 'fortieth', 'eighteenth', 'evilmerodach', 'months', 'expired']\nAspect 07:  ['trespassed', 'transgression', 'wickedly', 'punished', 'lewdness', 'erred', 'wickedness', 'witchcraft', 'sinned', 'thefts']\nAspect 08:  ['gave', 'took', 'followed', 'saw', 'slew', 'smote', 'sat', 'arose', 'ran', 'returned']\nAspect 09:  ['grape', 'baked', 'barrel', 'grapes', 'wheat', 'unsavoury', 'vine', 'parched', 'flour', 'butter']\nAspect 10:  ['dishes', 'greaves', 'swan', \"badgers'\", 'deer', 'inflammation', 'gier', 'flanks', 'pygarg', 'pelican']\nAspect 11:  ['ran', 'hasted', 'sat', 'arose', 'beheld', 'passed', 'met', 'kissed', 'returned', 'entered']\nAspect 12:  ['syrians', 'pursued', 'jericho', 'overtook', 'bethshittah', 'hit', 'garrison', 'overturned', 'along', 'landed']\nAspect 13:  ['pedahel', 'zebulunites', 'shuhamites', 'shiloni', 'asherites', 'azzan', 'landing', 'masrekah', 'mahali', 'shuham']\nAspect 14:  ['pharah', 'avim', 'hod', 'shilshah', 'remeth', 'beera', 'shamma', 'enhaddah', 'bethpazzez', 'amam']\nAspect 15:  ['rebekah', 'companion', 'rizpah', 'tamar', 'naomi', 'aiah', 'wife', 'sister', 'conceived', 'abigail']\nAspect 16:  ['escape', 'depart', 'flee', 'abide', 'get', 'enter', 'remain', 'swallow', 'bring', 'drive']\nAspect 17:  ['asswaged', 'interpreting', 'performing', 'wraths', 'shamhuth', 'uncorruptness', 'appeaseth', 'izrahite', 'asswage', 'zebulunites']\nAspect 18:  ['dreams', 'read', 'seer', 'learned', 'sayings', 'prophecy', 'understand', 'misused', 'answer', 'words']\nAspect 19:  ['sheweth', 'gentleness', 'saviour', 'glorious', 'longsuffering', 'unrighteousness', 'majesty', 'salvation', 'goodness', 'honour']\nAspect 20:  ['thirteen', 'zephon', 'phuvah', 'gadites', 'shuni', 'pahathmoab', 'issachar', 'shunites', 'shobai', 'talmon']\nAspect 21:  ['fatherless', 'poor', 'needy', 'widow', 'rich', 'stranger', 'taker', 'oppress', 'widows', 'giver']\nAspect 22:  ['clouds', 'heat', 'cloud', 'floods', 'tempest', 'smoke', 'overflowing', 'dark', 'thunder', 'waterspouts']\nAspect 23:  ['geshurites', 'amalekites', 'overtook', 'archi', 'coast', 'maachathites', 'bethjeshimoth', 'hamath', 'aroer', 'heshbon']\nAspect 24:  ['art', 'doest', 'heardest', 'sayest', 'spakest', 'wast', 'seest', 'sawest', 'saidst', 'knowest']\nAspect 25:  ['solemn', 'feasts', 'sacrifice', 'offer', 'sabbaths', 'hallow', 'thanksgiving', 'ordinance', 'sanctify', 'incense']\nAspect 26:  ['battle', 'war', 'fight', 'armed', 'syrians', 'armies', 'smelleth', 'fought', 'weapons', 'paweth']\nAspect 27:  ['robe', 'girded', 'mantle', 'beard', 'hem', 'fetters', 'towel', 'breeches', 'legs', 'purple']\nAspect 28:  ['magnify', 'prosper', 'rather', 'enemy', 'magnified', 'honor', 'thee', 'ashamed', 'preserve', 'saved']\nAspect 29:  ['killeth', 'hateth', 'gender', 'departeth', 'whoso', 'forecast', 'strifes', 'robbery', 'sinneth', 'brutish']\nAspect 30:  ['ignorance', 'falsely', 'perform', 'accept', 'hearers', 'justify', 'advantage', 'uncircumcision', 'observe', 'scythian']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Find K aspects in a Given Dataset using K-Means \"\"\"\n",
    "\n",
    "n_clusters = 30\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(bible_wv.vectors)\n",
    "centers = kmeans.cluster_centers_\n",
    "print('Initial aspects via K means:')\n",
    "for i in range(n_clusters):\n",
    "    print('Aspect {:02}: '.format(i+1), [word for word, _ in bible_wv.similar_by_vector(centers[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(data, dist=\"cosine\"):\n",
    "    if dist == \"cosine\":\n",
    "        mins = (cosine_distances(data) + np.eye(len(data)) * 1e5).min(axis=1)\n",
    "    elif dist == \"euclidean\":\n",
    "        mins = (euclidean_distances(data) + np.eye(len(data)) * 1e5).min(axis=1)\n",
    "    return np.square(np.var(mins)) / mins.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_emb = torch.from_numpy(bible_wv.vectors)\n",
    "a_emb = torch.from_numpy(centers)\n",
    "seed_w = None\n",
    "num_seeds = None\n",
    "lr = 0.005\n",
    "epochs = 50\n",
    "min_len = 1\n",
    "l = 0.1\n",
    "fix_a_emb = False\n",
    "\n",
    "net = AspectAutoencoder(vocab_size, embed_size, num_aspects=n_clusters, neg_samples=10, w_emb=w_emb, a_emb=a_emb, recon_method=\"centr\", seed_w=seed_w, num_seeds=num_seeds, attention=True, fix_w_emb=True, fix_a_emb=fix_a_emb)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    net = net.cuda()\n",
    "\n",
    "rec_loss = TripletMarginCosineLoss()\n",
    "orth_loss = OrthogonalityLoss()\n",
    "\n",
    "params = filter(lambda p: p.requires_grad, net.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Total Epoch:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ddb80b97fba94bdabe92ddac93358b94"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Epoch 0:   0%|          | 0/623 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c608fc1025ba4553a3090e2bb17c1f79"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Epoch 1:   0%|          | 0/623 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b71117c385294bb6809d825efb5e5908"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Epoch 2:   0%|          | 0/623 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "683dd539d0944b2e9465b9e381338061"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-32abb79c00cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mpositives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/ABAE/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, batch_num)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;31m# mask used for randomly selected negative examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_neg_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/ABAE/model.py\u001b[0m in \u001b[0;36m_create_neg_mask\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mneg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "for epoch in tqdm(range(epochs), desc='Total Epoch'):\n",
    "\n",
    "    for lines, ids in tqdm(dataloader, position=0, leave=False, desc='Epoch {}'.format(epoch)):\n",
    "        inputs = Variable(torch.Tensor(lines).long())\n",
    "\n",
    "        if inputs.shape[1] < min_len:\n",
    "            continue\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "\n",
    "        out, a_probs = net(inputs)\n",
    "\n",
    "        positives, negatives = net.get_targets()\n",
    "        loss = rec_loss(out, positives, negatives)\n",
    "\n",
    "        if not fix_a_emb:\n",
    "            aspects = net.get_aspects()\n",
    "            loss += l * orth_loss(aspects)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abae_centers = net.get_aspects().detach().cpu().numpy()\n",
    "\n",
    "print('Trained aspects via ABAE:')\n",
    "for i in range(n_clusters):\n",
    "    print('Aspect {:02}: '.format(i+1), [word for word, _ in bible_wv.similar_by_vector(abae_centers[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage(centers), coverage(abae_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = list()\n",
    "poss = list()\n",
    "aspects = list()\n",
    "recon_embedding = list()\n",
    "orign_embedding = list()\n",
    "for lines, ids in dataloader:\n",
    "    inputs = Variable(torch.Tensor(lines).long())\n",
    "\n",
    "    if inputs.shape[1] < min_len:\n",
    "        continue\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "\n",
    "    out, a_probs = net(inputs)\n",
    "\n",
    "    positives, negatives = net.get_targets()\n",
    "\n",
    "    outs.append(out.detach().cpu().numpy())\n",
    "    poss.append(positives.detach().cpu().numpy())\n",
    "    aspects.append(a_probs.detach().cpu().numpy())\n",
    "\n",
    "    recon_embedding.append(out.detach().cpu().numpy())\n",
    "    orign_embedding.append(positives.detach().cpu().numpy())\n",
    "\n",
    "outs = np.concatenate(outs, axis=0)\n",
    "poss = np.concatenate(poss, axis=0)\n",
    "aspects = np.concatenate(aspects, axis=0)\n",
    "\n",
    "cos_dis = list()\n",
    "for a, b in zip(outs, poss):\n",
    "    cos_dis.append(cosine_distances(a.reshape(1, -1), b.reshape(1, -1))[0, 0])\n",
    "print(sum(cos_dis), sum(cos_dis) / len(cos_dis))\n",
    "\n",
    "cos_dis_stat = [np.sum(np.array(cos_dis) <= i * 0.1) for i in range(1, 11)]\n",
    "print(cos_dis_stat)\n",
    "\n",
    "aspects_embedding = aspects.copy()\n",
    "aspects = np.argmax(aspects, axis=1)\n",
    "aspects_stat = [np.sum(aspects == i) for i in range(n_clusters)]\n",
    "print(aspects_stat)\n",
    "\n",
    "recon_embedding = np.concatenate(recon_embedding, axis=0)\n",
    "orign_embedding = np.concatenate(orign_embedding, axis=0)\n",
    "print(f\"Epoch {epoch+1}\", \"recon: \", coverage(recon_embedding, \"cosine\"), \"orign: \", coverage(orign_embedding, \"cosine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4), tight_layout=True)\n",
    "ax = sns.histplot(cos_dis, cumulative=True, bins=20, kde=True)\n",
    "ax.set(title=\"Cumulative Count\", xlabel=\"Cosine Distance\", ylabel=\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('verse2aspect.npy', aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('abae_centers.npy', abae_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}